from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline
from transformers import CLIPTokenizer
import os
import torch
import json

def is_sdxl_model(model_path):
    index_path = os.path.join(model_path, "model_index.json")
    if not os.path.exists(index_path):
        return False
    try:
        with open(index_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return any(k in json.dumps(data) for k in ["text_encoder_2", "tokenizer_2"])
    except Exception:
        return False

def load_model(model_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (SD –∏–ª–∏ SDXL)"""
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {model_path}")
    
    try:
        is_xl = is_sdxl_model(model_path)
        print(f"[DEBUG] –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑: {model_path}")
        print(f"[DEBUG] SDXL? ‚Üí {is_xl}")

        if is_xl:
            print("üîπ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ SDXL")
            pipe = StableDiffusionXLPipeline.from_pretrained(
                model_path,
                torch_dtype=torch.float32
            )
        else:
            print("üî∏ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—ã—á–Ω–æ–π –º–æ–¥–µ–ª–∏ SD")
            pipe = StableDiffusionPipeline.from_pretrained(
                model_path,
                torch_dtype=torch.float32,
                safety_checker=None
            )

        # üî• –û—Ç–∫–ª—é—á–∞–µ–º NSFW —Ñ–∏–ª—å—Ç—Ä –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫—É
        if hasattr(pipe, "safety_checker"):
            pipe.safety_checker = None
            pipe.requires_safety_checker = False
            print("‚ö†Ô∏è Safety checker –æ—Ç–∫–ª—é—á—ë–Ω.")

        if hasattr(pipe, "feature_extractor"):
            pipe.feature_extractor = None
            print("‚ö†Ô∏è Feature extractor –æ—Ç–∫–ª—é—á—ë–Ω.")

        # –ü–æ–¥–≥—Ä—É–∂–∞–µ–º tokenizer –≤—Ä—É—á–Ω—É—é –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        if not hasattr(pipe, "tokenizer") or pipe.tokenizer is None:
            try:
                print("‚ö†Ô∏è Tokenizer –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Ä—É—á–Ω—É—é...")
                pipe.tokenizer = CLIPTokenizer.from_pretrained(os.path.join(model_path, "tokenizer"))
                print("‚úÖ Tokenizer —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –≤—Ä—É—á–Ω—É—é.")
            except Exception as te:
                raise RuntimeError(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ tokenizer: {te}")

        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe = pipe.to(device)
        pipe.set_progress_bar_config(disable=True)
        print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device.upper()}")
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
        print(f"DEBUG: is_xl={is_xl}")
        if hasattr(pipe, "vae"):
            print("‚úÖ VAE –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
        return pipe

    except Exception as e:
        raise RuntimeError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
